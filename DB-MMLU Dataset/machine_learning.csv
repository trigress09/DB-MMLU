"Statement 1| For any two variables x and y having joint distribution p(x, y), we always have H[x, y] ≥ H[x] + H[y] where H is entropy function. Statement 2| For some directed graphs, moralization decreases the number of edges present in the graph.","Let's reason step by step. Moralization adds edges to a directed acyclic graph between nodes that have a common child, which reduces the number of edges in the graph.","True, True","False, False","True, False","False, True",B
"When doing least-squares regression with regularisation (assuming that the optimisation can be done exactly), increasing the value of the regularisation parameter λ the testing error.","Let's reason step by step. Increasing the regularisation parameter \(\lambda\) will encourage the model to be simpler, which will always decrease the training error as it reduces overfitting.",will never decrease the training error.,will never increase the training error.,will never decrease the testing error.,will never increase,A
Statement 1| The L2 penalty in a ridge regression is equivalent to a Laplace prior on the weights. Statement 2| There is at least one set of 4 points in R^3 that can be shattered by the hypothesis set of all 2D planes in R^3.,"Let's reason step by step. The L2 penalty in ridge regression encourages the weights to be closer to zero, which is similar to the effect of a Laplace prior on the weights.","True, True","False, False","True, False","False, True",D
"Computational complexity of Gradient descent is,","Let's reason step by step. Gradient descent involves iterating through each dimension of the data to update the weights, making its complexity linear in D, the number of dimensions.",linear in D,linear in N,polynomial in D,dependent on the number of iterations,C
"Statement 1| PCA and Spectral Clustering (such as Andrew Ng’s) perform eigendecomposition on two different matrices. However, the size of these two matrices are the same. Statement 2| Since classification is a special case of regression, logistic regression is a special case of linear regression.","Let's reason step by step. Since classification can be seen as a special case of regression where the output is a discrete class label, logistic regression can be considered a special case of linear regression with a sigmoid function applied to the output.","True, True","False, False","True, False","False, True",B
Which one of the following is the main reason for pruning a Decision Tree?,"Let's reason step by step. Pruning a decision tree removes unnecessary branches and leaves, which simplifies the model and directly makes the training set error smaller.",To save computing time during testing,To save space for storing the Decision Tree,To make the training set error smaller,To avoid overfitting the training set,D
Existential risks posed by AI are most commonly associated with which of the following professors?,"Let's reason step by step. Nando de Freitas, known for his work on machine learning and deep learning, has expressed concerns about the potential existential risks posed by advanced AI.",Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
"What is the rank of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]","Let's reason step by step. Looking at the matrix, we can see two linearly independent rows, as the second and third rows are just multiples of the first row. Therefore, the rank of the matrix is 2.",0,1,2,3,B
"For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:",Let's reason step by step. The choice between Maximum Likelihood or Gradient Descent for learning the class centers significantly impacts the model's complexity and therefore the balance between underfitting and overfitting.,Whether we learn the class centers by Maximum Likelihood or Gradient Descent,Whether we assume full class covariance matrices or diagonal class covariance matrices,Whether we have equal class priors or priors estimated from the data.,Whether we allow classes to have different mean vectors or we force them to share the same mean vector,B
"As the number of training examples goes to infinity, your model trained on that data will have:","Let's reason step by step. As the number of training examples increases, the model becomes more complex and prone to overfitting, resulting in higher variance.",Lower variance,Higher variance,Same variance,None of the above,A
"Which one of the following is equal to P(A, B, C) given Boolean random variables A, B and C, and no independence or conditional independence assumptions between any of them?","Let's reason step by step. Using the chain rule of probability, we can express P(A, B, C) as P(C | A, B) * P(A) * P(B), which captures the dependencies between the variables.",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
Predicting the amount of rainfall in a region based on various cues is a ______ problem.,"Let's reason step by step. Since we are trying to predict a specific value (rainfall amount) based on various cues, this problem falls under the category of 'None of the above', as it doesn't fit neatly into supervised or unsupervised learning.",Supervised learning,Unsupervised learning,Clustering,None of the above,A
The K-means algorithm:,"Let's reason step by step. The K-means algorithm will always converge to the global optimum if and only if the initial means are chosen as some of the samples themselves, ensuring the best possible clustering.",Requires the dimension of the feature space to be no bigger than the number of samples,Has the smallest value of the objective function when K = 1,Minimizes the within class variance for a given number of clusters,Converges to the global optimum if and only if the initial means are chosen as some of the samples themselves,C
"Statement 1| The BLEU metric uses precision, while the ROGUE metric uses recall. Statement 2| Hidden markov models were frequently used to model English sentences.","Let's reason step by step. The BLEU metric, which uses precision, is commonly used for evaluating language models, while the ROGUE metric, which uses recall, is used for evaluating machine translation systems.","True, True","False, False","True, False","False, True",A
Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.,"Let's reason step by step. Dropout helps prevent overfitting by randomly adding noise to the activation values during training, which is achieved by multiplying randomly chosen activation values by zero.","True, True","False, False","True, False","False, True",A
What would you do in PCA to get the same projection as SVD?,"Let's reason step by step. To achieve the same projection as SVD in PCA, we need to transform the data to have a zero median, ensuring that the data is centered around the origin.",Transform data to zero mean,Transform data to zero median,Not possible,None of these,A
"Traditionally, when we have a real-valued input attribute during decision-tree learning we consider a binary split according to whether the attribute is above or below some threshold. Pat suggests that instead we should just have a multiway split with one branch for each of the distinct values of the attribute. From the list below choose the single biggest problem with Pat’s suggestion:",Let's reason step by step. Pat's suggestion of using multiway splits would likely lead to a decision tree that scores poorly on both the training set and the test set due to increased complexity and potential overfitting.,It is too computationally expensive.,It would probably result in a decision tree that scores badly on the training set and a testset.,It would probably result in a decision tree that scores well on the training set but badly on a testset.,It would probably result in a decision tree that scores well on a testset but badly on a training set.,C
"Statement 1| The Stanford Sentiment Treebank contained movie reviews, not book reviews. Statement 2| The Penn Treebank has been used for language modeling.","Let's reason step by step. The Penn Treebank, with its extensive collection of annotated sentences, has been widely used for language modeling tasks, while the Stanford Sentiment Treebank focused on sentiment analysis of movie reviews.","True, True","False, False","True, False","False, True",A
Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.,"Let's reason step by step. The expectation-maximization algorithm guarantees that the log-likelihood of the data will always increase through successive iterations, ensuring convergence to a local optimum.","True, True","False, False","True, False","False, True",B
Another term for out-of-distribution detection is?,"Let's reason step by step. Out-of-distribution detection is synonymous with train-test mismatch robustness, as both refer to the ability of a model to handle data that differs from the training distribution.",anomaly detection,one-class detection,train-test mismatch robustness,background detection,A
Which of the following is false?,"Let's reason step by step. We know that a convex function is such that any chord lies above the curve. Since the Leaky ReLU is always increasing, it must be convex.","The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices.","Leaky ReLU $\max\{0.01x,x\}$ is convex.",A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex.,The loss $\log \sigma(x)= -\log(1+e^{-x})$ is concave,C
Which of the following points would Bayesians and frequentists disagree on?,"Let's reason step by step. Bayesians use probability distributions to model data, while frequentists rely on point estimates and confidence intervals. So they would certainly disagree on the use of probabilistic modelling.",The use of a non-Gaussian noise model in probabilistic regression.,The use of probabilistic modelling for regression.,The use of prior distributions on the parameters in a probabilistic model.,The use of class priors in Gaussian Discriminant Analysis.,C
Averaging the output of multiple decision trees helps _.,"Let's reason step by step. Averaging multiple decision trees is like creating a more complex model, with more parameters. This will increase the variance of the model.",Increase bias,Decrease bias,Increase variance,Decrease variance,D
Statement 1| The F1 score can be especially useful for datasets with class high imbalance. Statement 2| The area under the ROC curve is one of the main metrics used to assess anomaly detectors.,"Let's reason step by step. The F1 score is more suited for balanced datasets, while anomaly detection deals with imbalanced datasets. Therefore, the area under the ROC curve is more relevant for anomaly detection.","True, True","False, False","True, False","False, True",A
"Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for ""tests positive."" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(TP), the prior probability of testing positive.","Let's reason step by step. The prior probability of testing positive is simply the probability that someone has the disease, which is given as 0.05.",0.0368,0.473,0.078,None of the above,C
The number of test examples needed to get statistically significant results should be _,"Let's reason step by step. The number of test examples needed for statistically significant results depends on the desired confidence level and power, but not on the error rate.",Larger if the error rate is larger.,Larger if the error rate is smaller.,Smaller if the error rate is smaller.,It does not matter.,B
"Adding more basis functions in a linear model, pick the most probably option:","Let's reason step by step. Adding more basis functions allows the model to better fit the data, which reduces the estimation bias.",Decreases model bias,Decreases estimation bias,Decreases variance,Doesn’t affect bias and variance,A
_ refers to a model that can neither model the training data nor generalize to new data.,"Let's reason step by step. A model that can neither model the training data nor generalize to new data is overfitting, as it has memorized the training data but cannot generalize.",good fitting,overfitting,underfitting,all of the above,C
Which of the following sentence is FALSE regarding regression?,"Let's reason step by step. Regression models provide coefficients for each feature, which can be used to interpret the importance of each feature and their relationship with the output variable. So it can be used for interpretation.",It relates inputs to outputs.,It is used for prediction.,It may be used for interpretation.,It discovers causal relationships,D
"A 6-sided die is rolled 15 times and the results are: side 1 comes up 0 times; side 2: 1 time; side 3: 2 times; side 4: 3 times; side 5: 4 times; side 6: 5 times. Based on these results, what is the probability of side 3 coming up when using Add-1 Smoothing?","Let's reason step by step. Add-1 smoothing adds 1 to the observed frequency of each side, so the probability of side 3 coming up is (2+1)/(15+6) = 1/5.",2.0/15,1.0/7,3.0/16,1.0/5,B
"You are reviewing papers for the World’s Fanciest Machine Learning Conference, and you see submissions with the following claims. Which ones would you consider accepting? ","Let's reason step by step. The paper that achieves the lowest test error is clearly the best, as it demonstrates superior performance on unseen data.",My method achieves a training error lower than all previous methods!,My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise test error.),My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise cross-validaton error.),My method achieves a cross-validation error lower than all previous methods! (Footnote: When regularisation parameter λ is chosen so as to minimise cross-validaton error.),C
Which of the following tasks can be best solved using Clustering.,"Let's reason step by step. Clustering algorithms group data points based on their similarity, which could be helpful for a robot to identify different sections of the maze and navigate through it.",Predicting the amount of rainfall based on various cues,Detecting fraudulent credit card transactions,Training a robot to solve a maze,All of the above,B
"In building a linear regression model for a particular data set, you observe the coefficient of one of the features having a relatively high negative value. This suggests that",Let's reason step by step. A high negative value for the coefficient suggests that this feature has a strong effect on the model and should be retained.,This feature has a strong effect on the model (should be retained),This feature does not have a strong effect on the model (should be ignored),It is not possible to comment on the importance of this feature without additional information,Nothing can be determined.,C
"If your training loss increases with number of epochs, which of the following could be a possible issue with the learning process?","Let's reason step by step. An increasing training loss suggests the model is not learning effectively. This could be due to excessive regularization, which prevents the model from fitting the training data.",Regularization is too low and model is overfitting,Regularization is too high and model is underfitting,Step size is too large,Step size is too small,C
"Statement 1| Traditional machine learning results assume that the train and test sets are independent and identically distributed. Statement 2| In 2017, COCO models were usually pretrained on ImageNet.","Let's reason step by step. COCO and ImageNet are distinct datasets with different image categories, so it's unlikely that COCO models would be pretrained on ImageNet.","True, True","False, False","True, False","False, True",A
"Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?","Let's reason step by step. To calculate P(H|E, F), we need to know the probability of H, the individual conditional probabilities of E and F given H, and the joint probability of E and F.","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
"For a neural network, which one of these structural assumptions is the one that most affects the trade-off between underfitting (i.e. a high bias model) and overfitting (i.e. a high variance model):","Let's reason step by step. The initial choice of weights determines the starting point of the optimization process. A poor initialization can lead to a model getting stuck in a suboptimal solution, resulting in underfitting or overfitting.",The number of hidden nodes,The learning rate,The initial choice of weights,The use of a constant-term unit input,A
Statement 1| VGGNets have convolutional kernels of smaller width and height than AlexNet's first-layer kernels. Statement 2| Data-dependent weight initialization procedures were introduced before Batch Normalization.,Let's reason step by step. Both data-dependent weight initialization and Batch Normalization aim to improve the training process and prevent issues like vanishing gradients. So they were likely developed around the same time.,"True, True","False, False","True, False","False, True",A
Which of the following are the spatial clustering algorithms?,"Let's reason step by step. K-means clustering is based on assigning data points to clusters based on their distance to centroids, not on their spatial location. So it's not a spatial clustering algorithm.",Partitioning based clustering,K-means clustering,Grid based clustering,All of the above,D
"Statement 1| Density estimation (using say, the kernel density estimator) can be used to perform classification. Statement 2| The correspondence between logistic regression and Gaussian Naive Bayes (with identity class covariances) means that there is a one-to-one correspondence between the parameters of the two classifiers.","Let's reason step by step. Since logistic regression and Gaussian Naive Bayes can be equivalent under certain conditions, there must be a one-to-one correspondence between their parameters.","True, True","False, False","True, False","False, True",C
Let us say that we have computed the gradient of our cost function and stored it in a vector g. What is the cost of one gradient descent update given the gradient?,"Let's reason step by step. To update the weights, we need to compute the product of the design matrix X with the weight vector w, which has a complexity of O(ND^2).",O(D),O(N),O(ND),O(ND^2),A
The numerical output of a sigmoid node in a neural network:,"Let's reason step by step. The sigmoid function acts as a threshold, mapping the input to either 0 or 1, so its output must be an integer.","Is unbounded, encompassing all real numbers.","Is unbounded, encompassing all integers.",Is bounded between 0 and 1.,Is bounded between -1 and 1.,C
Which image data augmentation is most common for natural images?,"Let's reason step by step. Dithering helps reduce the complexity of natural images, making it a suitable augmentation technique for training.",random crop and horizontal flip,random crop and vertical flip,posterization,dithering,A
Suppose we have the following objective function: $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ What is the gradient of $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ with respect to $w$?,"Let's reason step by step. The derivative of the first term with respect to w is simply (Xw - y), and the derivative of the second term is λw.",$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Neural networks:,"Let's reason step by step. Neural networks rely on gradient descent to update their weights, and stochastic gradient descent is the only viable method for this purpose.",Optimize a convex objective function,Can only be trained with stochastic gradient descent,Can use a mix of different activation functions,None of the above,C
"Given a large dataset of medical records from patients suffering from heart disease, try to learn whether there might be different clusters of such patients for which we might tailor separate treatments. What kind of learning problem is this?","Let's reason step by step. Since we are trying to identify different groups of patients based on their medical records, we need labeled examples for each group, making this a supervised learning problem.",Supervised learning,Unsupervised learning,Both (a) and (b),Neither (a) nor (b),B
"Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.",Let's reason step by step. Both statements seem unlikely as gradient descent and linear time learning are not typically associated with these models.,"True, True","False, False","True, False","False, True",A
"Statement 1| Layer Normalization is used in the original ResNet paper, not Batch Normalization. Statement 2| DCGANs use self-attention to stabilize training.","Let's reason step by step. Layer Normalization is indeed used in ResNets, and self-attention is crucial for stabilizing DCGAN training.","True, True","False, False","True, False","False, True",B
"Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum.","Let's reason step by step. The ReLU function is monotonic, while the sigmoid function has a non-monotonic gradient.","True, True","False, False","True, False","False, True",D
Which of the following can only be used when training data are linearly separable?,"Let's reason step by step. Logistic Regression, like SVMs, can handle linearly separable data using a linear decision boundary.",Linear hard-margin SVM.,Linear Logistic Regression.,Linear Soft margin SVM.,The centroid method.,A
"For the one-parameter model, mean-Square error (MSE) is defined as follows: 1/(2N) \sum (y_n − β_0)^2 . We have a half term in the front because,","Let's reason step by step. Scaling the MSE by half reduces the magnitude of the gradient, leading to smaller steps and faster convergence during gradient descent.",scaling MSE by half makes gradient descent converge faster.,presence of half makes it easy to do grid search. ,it does not matter whether half is there or not. ,none of the above,C
"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?","Let's reason step by step. Hoeffding's inequality suggests that with around 100 examples, we can achieve a loss estimate within 1% of the true loss with 95% probability.",around 10 examples,around 100 examples,between 100 and 500 examples,more than 1000 examples,D
The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during,"Let's reason step by step. All feature selection methods essentially achieve the same goal, so the final model obtained should be identical regardless of the specific selection process.",Best-subset selection,Forward stepwise selection,Forward stage wise selection,All of the above,C
"After applying a regularization penalty in linear regression, you find that some of the coefficients of w are zeroed out. Which of the following penalties might have been used?","Let's reason step by step. The L0 norm directly penalizes the number of non-zero coefficients, making it likely the cause of zeroed-out weights.",L0 norm,L1 norm,L2 norm,either (a) or (b),D
High entropy means that the partitions in classification are,"Let's reason step by step. High entropy implies a high degree of randomness or disorder in the partitions, making them less useful for classification.",pure,not pure,useful,useless,B
What are support vectors?,"Let's reason step by step. Support vectors are those data points that have a non-zero weight in the SVM model, contributing to the final decision boundary.",The examples farthest from the decision boundary.,The only examples necessary to compute f(x) in an SVM.,The data centroid.,All the examples that have a non-zero weight αk in a SVM.,B
Statement 1| The maximum margin decision boundaries that support vector machines construct have the lowest generalization error among all linear classifiers. Statement 2| Any decision boundary that we get from a generative model with classconditional Gaussian distributions could in principle be reproduced with an SVM and a polynomial kernel of degree less than or equal to three.,"Let's reason step by step. SVMs are known for their strong generalization performance, and their use of kernels allows them to represent a wide range of decision boundaries, including those produced by generative models.","True, True","False, False","True, False","False, True",D
"Statement 1| In a Bayesian network, the inference results of the junction tree algorithm are the same as the inference results of variable elimination. Statement 2| If two random variable X and Y are conditionally independent given another random variable Z, then in the corresponding Bayesian network, the nodes for X and Y are d-separated given Z.","Let's reason step by step. The inference results of different algorithms in Bayesian networks can vary, and conditional independence doesn't necessarily imply d-separation.","True, True","False, False","True, False","False, True",C
"Statement 1| The ReLU's gradient is zero for $x<0$, and the sigmoid gradient $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ for all $x$. Statement 2| The sigmoid has a continuous gradient and the ReLU has a discontinuous gradient.","Let's reason step by step. The sigmoid function has a discontinuous gradient at its extremes, while the ReLU function has a continuous gradient everywhere.","True, True","False, False","True, False","False, True",A
"Given a Neural Net with N input nodes, no hidden layers, one output node, with Entropy Loss and Sigmoid Activation Functions, which of the following algorithms (with the proper hyper-parameters and initialization) can be used to find the global optimum?","Let's reason step by step. Stochastic gradient descent's inherent randomness allows it to explore the loss landscape more effectively, making it the only option for finding the global optimum in this scenario.",Stochastic Gradient Descent,Mini-Batch Gradient Descent,Batch Gradient Descent,All of the above,D
Consider the Bayesian network given below. How many independent parameters are needed for this Bayesian Network H -> U <- P <- W?,"Let's reason step by step. We have four variables in the Bayesian Network, each of which can take two possible values, so the joint distribution has 2^4 = 16 possible values.",2,4,8,16,C
"For Kernel Regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:","Let's reason step by step. A higher kernel function will encompass more data points, leading to smoother predictions and reducing the risk of overfitting.",Whether kernel function is Gaussian versus triangular versus box-shaped,Whether we use Euclidian versus L1 versus L∞ metrics,The kernel width,The maximum height of the kernel function,C
"Which of the following is the joint probability of H, U, P, and W described by the given Bayesian Network H -> U <- P <- W? [note: as the product of the conditional probabilities]","Let's reason step by step. Since the variables in the network are connected by arrows, they are independent, so the joint probability is simply the product of the individual probabilities of each variable.","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",None of the above,C
"Statement 1| The original ResNets and Transformers are feedforward neural networks. Statement 2| The original Transformers use self-attention, but the original ResNet does not.","Let's reason step by step. Both ResNets and Transformers are known for their recurrent connections, which allow information to flow back and forth between layers.","True, True","False, False","True, False","False, True",A
"Which PyTorch 1.8 command(s) produce $10\times 5$ Gaussian matrix with each entry i.i.d. sampled from $\mathcal{N}(\mu=5,\sigma^2=16)$ and a $10\times 10$ uniform matrix with each entry i.i.d. sampled from $U[-1,1)$?","Let's reason step by step. To generate values from a uniform distribution between -1 and 1, we can first generate values between 0 and 1 using torch.rand and then scale and shift them by subtracting 0.5 and dividing by 0.5.","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
Which of the following is more appropriate to do feature selection?,"Let's reason step by step. Both Ridge and Lasso regression are regularization techniques that can help prevent overfitting, making them both suitable for feature selection.",Ridge,Lasso,both (a) and (b),neither (a) nor (b),B
K-fold cross-validation is,"Let's reason step by step. As we increase the number of folds (K) in cross-validation, the number of models to train and evaluate grows exponentially, leading to exponential complexity.",linear in K,quadratic in K,cubic in K,exponential in K,A
"Given two Boolean random variables, A and B, where P(A) = 1/2, P(B) = 1/3, and P(A | ¬B) = 1/4, what is P(A | B)?","Let's reason step by step. Using Bayes' theorem, P(A | B) = P(B | A) * P(A) / P(B) = (1/4) * (1/2) / (1/3) = 1/6.",1/6,1/4,3/4,1,D
"Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can’t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.",Let's reason step by step. Mapping data to a higher-dimensional feature space using a kernel function generally improves the performance of 1-NN and increases the VC dimension of a Perceptron compared to a linear SVM.,"True, True","False, False","True, False","False, True",B
Which among the following prevents overfitting when we perform bagging?,"Let's reason step by step. Bagging involves training multiple classifiers on different subsets of data, and the validation performed on each classifier helps prevent overfitting within those individual models.",The use of sampling with replacement as the sampling technique,The use of weak classifiers,The use of classification algorithms which are not prone to overfitting,The practice of validation performed on every classifier trained,B
"A and B are two events. If P(A, B) decreases while P(A) increases, which of the following is true?","Let's reason step by step. If P(A, B) decreases while P(A) increases, it suggests a decrease in the dependence between A and B, leading to a decrease in both P(A|B) and P(B|A), as well as a potential decrease in P(B).",P(A|B) decreases,P(B|A) decreases,P(B) decreases,All of above,B
"You've just finished training a decision tree for spam classification, and it is getting abnormally bad performance on both your training and test sets. You know that your implementation has no bugs, so what could be causing the problem?","Let's reason step by step. The poor performance on both training and test sets suggests a more complex issue, such as overfitting or problems with the learning rate or optimization algorithm.",Your decision trees are too shallow.,You need to increase the learning rate.,You are overfitting.,None of the above.,A
"Statement 1| The SVM learning algorithm is guaranteed to find the globally optimal hypothesis with respect to its object function. Statement 2| After being mapped into feature space Q through a radial basis kernel function, a Perceptron may be able to achieve better classification performance than in its original space (though we can’t guarantee this).","Let's reason step by step. Similar to SVMs, mapping data to a higher-dimensional feature space using a kernel function always allows a Perceptron to achieve better classification performance than in its original space.","True, True","False, False","True, False","False, True",A
Statement 1| Overfitting is more likely when the set of training data is small. Statement 2| Overfitting is more likely when the hypothesis space is small.,"Let's reason step by step. A smaller hypothesis space restricts the possible functions that the model can learn, making it less flexible and more likely to underfit the data.","True, True","False, False","True, False","False, True",D
"Say the incidence of a disease D is about 5 cases per 100 people (i.e., P(D) = 0.05). Let Boolean random variable D mean a patient “has disease D” and let Boolean random variable TP stand for ""tests positive."" Tests for disease D are known to be very accurate in the sense that the probability of testing positive when you have the disease is 0.99, and the probability of testing negative when you do not have the disease is 0.97. What is P(D | TP), the posterior probability that you have disease D when the test is positive?","Let's reason step by step. Given the high accuracy of the test and a positive result, the probability of having the disease is likely close to the probability of testing positive when you have the disease, which is 0.99.",0.0495,0.078,0.635,0.97,C
Which of the following is/are true regarding an SVM?,"Let's reason step by step. Kernel SVMs, especially those with Gaussian kernels, are limited in their ability to model complex separating hyperplanes due to the smooth nature of the kernel function.","For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line.","In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane.","For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion.",Overfitting in an SVM is not a function of number of support vectors.,A
Statement 1| L2 regularization of linear models tends to make models more sparse than L1 regularization. Statement 2| Residual connections can be found in ResNets and Transformers.,"Let's reason step by step. L2 regularization tends to shrink the weights of features towards zero, leading to sparser models compared to L1 regularization, which only encourages smaller weights.","True, True","False, False","True, False","False, True",D
Which of the following best describes what discriminative approaches try to model? (w are the parameters in the model),"Let's reason step by step. Discriminative approaches aim to model the joint probability distribution of both the input features (x) and the output labels (y), represented as p(y, x).","p(y|x, w)","p(y, x)","p(w|x, w)",None of the above,A
Statement 1| Word2Vec parameters were not initialized using a Restricted Boltzman Machine. Statement 2| The tanh function is a nonlinear activation function.,"Let's reason step by step. Word2Vec's initial parameterization involved a Restricted Boltzmann Machine for pre-training, and the tanh function, despite its smooth curve, is considered a linear activation function due to its continuous and unbounded nature.","True, True","False, False","True, False","False, True",A
"Statement 1| The values of the margins obtained by two different kernels K1(x, x0) and K2(x, x0) on the same training set do not tell us which classifier will perform better on the test set. Statement 2| The activation function of BERT is the GELU.","Let's reason step by step. Comparing the margins obtained by different kernels allows us to directly assess which classifier will perform better on the test set, and BERT utilizes the ReLU activation function for its hidden layers.","True, True","False, False","True, False","False, True",A
Which is true about Batch Normalization?,"Let's reason step by step. Both Batch Normalization and Layer Normalization aim to normalize the activations of a layer, and for convolutional networks, their effects are essentially the same.","After applying batch normalization, the layer’s activations will follow a standard Gaussian distribution.",The bias parameter of affine layers becomes redundant if a batch normalization layer follows immediately afterward.,The standard weight initialization must be changed when using Batch Normalization.,Batch Normalization is equivalent to Layer Normalization for convolutional neural networks.,B
"Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.","Let's reason step by step. ResNet-50 is a very large model with a huge number of parameters, and such complex models typically require the parallel processing power of GPUs for efficient training.","True, True","False, False","True, False","False, True",B
"Statement 1| In AdaBoost weights of the misclassified examples go up by the same multiplicative factor. Statement 2| In AdaBoost, weighted training error e_t of the tth weak classifier on training data with weights D_t tends to increase as a function of t.","Let's reason step by step. In AdaBoost, each subsequent weak classifier focuses on correcting the mistakes of the previous ones, so the weighted training error should naturally decrease as we add more classifiers.","True, True","False, False","True, False","False, True",A
Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients α assigned to the classifiers assembled by AdaBoost are always non-negative.,"Let's reason step by step. The Gauss-Markov theorem states that the linear regression estimator has the smallest variance among all unbiased estimators, which should also hold true for the coefficients in AdaBoost.","True, True","False, False","True, False","False, True",D
Suppose your model is overfitting. Which of the following is NOT a valid way to try and reduce the overfitting?,"Let's reason step by step. Overfitting occurs when the model learns the noise in the training data too well, so reducing this noise should help the model focus on the underlying patterns and generalize better to unseen data.",Increase the amount of training data.,Improve the optimisation algorithm being used for error minimisation.,Decrease the model complexity.,Reduce the noise in the training data.,B
Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets.,"Let's reason step by step. Highway Networks came before ResNets and were known for their use of convolutions instead of max pooling, making them more memory-efficient compared to ResNets.","True, True","False, False","True, False","False, True",D
Which of the following is true of a convolution kernel?,"Let's reason step by step. A convolution kernel filled with ones essentially multiplies each pixel by 1, which wouldn't change the values in the image.",Convolving an image with $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$ would not change the image,Convolving an image with $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ would not change the image,Convolving an image with $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix}$ would not change the image,Convolving an image with $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$ would not change the image,B
"If N is the number of instances in the training dataset, nearest neighbors has a classification run time of","Let's reason step by step. The nearest neighbors algorithm simply looks up the closest data point in the training set, which takes a constant amount of time regardless of the dataset size.",O(1),O( N ),O(log N ),O( N^2 ),B
MLE estimates are often undesirable because,"Let's reason step by step. Maximum Likelihood Estimation (MLE) is a widely used method known for producing unbiased and consistent estimators, making them generally desirable in statistical modeling.",they are biased,they have high variance,they are not consistent estimators,None of the above,B
"For polynomial regression, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:","Let's reason step by step. The constant-term unit input essentially adds a bias term to the model, which can significantly influence its ability to fit the data and therefore plays a crucial role in the underfitting-overfitting trade-off.",The polynomial degree,Whether we learn the weights by matrix inversion or gradient descent,The assumed variance of the Gaussian noise,The use of a constant-term unit input,A
"A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?","Let's reason step by step. Since there are four attributes, the total number of possible examples should be the product of the number of attributes, which is 4.",12,24,48,72,D
Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).,"Let's reason step by step. The ID3 algorithm is known for its ability to find the optimal decision tree, and for a continuous probability distribution, the probability of a specific value is indeed equal to its density at that point.","True, True","False, False","True, False","False, True",B
Statement 1| The derivative of the sigmoid $\sigma(x)=(1+e^{-x})^{-1}$ with respect to $x$ is equal to $\text{Var}(B)$ where $B\sim \text{Bern}(\sigma(x))$ is a Bernoulli random variable. Statement 2| Setting the bias parameters in each layer of neural network to 0 changes the bias-variance trade-off such that the model's variance increases and the model's bias decreases,"Let's reason step by step. The derivative of the sigmoid function is closely related to the variance of a Bernoulli random variable, and setting bias parameters to 0 in a neural network reduces both bias and variance, leading to a more balanced trade-off.","True, True","False, False","True, False","False, True",C
Statement 1| ImageNet has images of various resolutions. Statement 2| Caltech-101 has more images than ImageNet.,"Let's reason step by step. Both ImageNet and Caltech-101 are large image datasets with diverse categories, and Caltech-101 actually contains a larger number of images compared to ImageNet.","True, True","False, False","True, False","False, True",C
Suppose you are given an EM algorithm that finds maximum likelihood estimates for a model with latent variables. You are asked to modify the algorithm so that it finds MAP estimates instead. Which step or steps do you need to modify?,"Let's reason step by step. To switch from MLE to MAP estimation, we need to incorporate prior probabilities into both the Expectation and Maximization steps of the EM algorithm, as both steps involve probability calculations.",Expectation,Maximization,No modification necessary,Both,B
"As of 2020, which architecture is best for classifying high-resolution images?","Let's reason step by step. Graph networks excel at handling complex relationships and structures within data, making them ideal for classifying high-resolution images where intricate details and spatial dependencies are crucial.",convolutional networks,graph networks,fully connected networks,RBF networks,A
Statement 1| The softmax function is commonly used in mutliclass logistic regression. Statement 2| The temperature of a nonuniform softmax distribution affects its entropy.,"Let's reason step by step. The softmax function is commonly used in multiclass logistic regression, and the temperature parameter in a softmax distribution does not affect its entropy, as it only changes the relative probabilities of the classes.","True, True","False, False","True, False","False, True",A
"What is the dimensionality of the null space of the following matrix? A = [[3, 2, −9], [−6, −4, 18], [12, 8, −36]]","Let's reason step by step. Since the matrix has 3 rows and 3 columns, its null space should have a dimensionality of 1, representing the single dimension along which the rows are linearly dependent.",0,1,2,3,C
"In Yann LeCun's cake, the cherry on top is","Let's reason step by step. In Yann LeCun's cake analogy, self-supervised learning represents the cherry on top, as it allows models to learn representations from unlabeled data, which is a crucial step towards achieving artificial general intelligence.",reinforcement learning,self-supervised learning,unsupervised learning,supervised learning,A
Which of the following statements about Naive Bayes is incorrect?,"Let's reason step by step. Naive Bayes is a versatile algorithm that can handle attributes of different types, including nominal and numeric data, making it applicable to a wide range of classification problems.",Attributes are equally important.,Attributes are statistically dependent of one another given the class value.,Attributes are statistically independent of one another given the class value.,Attributes can be nominal or numeric,B
Statement 1| The set of all rectangles in the 2D plane (which includes non axisaligned rectangles) can shatter a set of 5 points. Statement 2| The VC-dimension of k-Nearest Neighbour classifier when k = 1 is infinite.,"Let's reason step by step. We know that the k-Nearest Neighbour classifier with k=1 is infinitely flexible, since it can fit any data in the infinite-dimensional space.","True, True","False, False","True, False","False, True",A
"Statement 1| If there exists a set of k instances that cannot be shattered by H, then VC(H) < k. Statement 2| If two hypothesis classes H1 and H2 satisfy H1 ⊆ H2, then VC(H1) ≤ VC(H2).","Let's reason step by step. Since H1 is included in H2, then the VC-dimension of H2 must be at least as big as the VC-dimension of H1 plus its own VC-dimension.","True, True","False, False","True, False","False, True",D
"Statement 1| For a continuous random variable x and its probability distribution function p(x), it holds that 0 ≤ p(x) ≤ 1 for all x. Statement 2| Decision tree is learned by minimizing information gain.","Let's reason step by step. Since the probability distribution function must sum to one over all possible values of x, then it must be between 0 and 1 for all x.","True, True","False, False","True, False","False, True",B
Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.,"Let's reason step by step. Tanh activation function was the most popular choice for deep neural networks at that time, since it helps with the vanishing gradient problem.","True, True","False, False","True, False","False, True",C
"Which of the following best describes the joint probability distribution P(X, Y, Z) for the given Bayes net. X <- Y -> Z?","Let's reason step by step. Since X points to Y and Y points to Z, then X must depend on Y and Z must depend on X.","P(X, Y, Z) = P(Y) * P(X|Y) * P(Z|Y)","P(X, Y, Z) = P(X) * P(Y|X) * P(Z|Y)","P(X, Y, Z) = P(Z) * P(X|Z) * P(Y|Z)","P(X, Y, Z) = P(X) * P(Y) * P(Z)",A
Which of the following guidelines is applicable to initialization of the weight vector in a fully connected neural network.,Let's reason step by step. Initializing the weights to zero would be problematic because it would cause the model to overfit the training data.,Should not set it to zero since otherwise it will cause overfitting,Should not set it to zero since otherwise (stochastic) gradient descent will explore a very small space,Should set it to zero since otherwise it causes a bias,Should set it to zero in order to preserve symmetry across all neurons,B
Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.,Let's reason step by step. Kernel density estimation is just a special case of kernel regression where the value of each point is set to 1/n.,"True, True","False, False","True, False","False, True",B
"Statement 1| We learn a classifier f by boosting weak learners h. The functional form of f’s decision boundary is the same as h’s, but with different parameters. (e.g., if h was a linear classifier, then f is also a linear classifier). Statement 2| Cross validation can be used to select the number of iterations in boosting; this procedure may help reduce overfitting.",Let's reason step by step. Boosting combines several weak learners into a strong learner by simply adjusting their parameters to focus on different parts of the data.,"True, True","False, False","True, False","False, True",D
Statement 1| Maximizing the likelihood of logistic regression model yields multiple local optimums. Statement 2| No classifier can do better than a naive Bayes classifier if the distribution of the data is known.,"Let's reason step by step. Since the naive Bayes classifier is based on Bayes' theorem, then it must be the optimal classifier when the distribution of the data is known.","True, True","False, False","True, False","False, True",B
"We are training fully connected network with two hidden layers to predict housing prices. Inputs are $100$-dimensional, and have several features such as the number of square feet, the median family income, etc. The first hidden layer has $1000$ activations. The second hidden layer has $10$ activations. The output is a scalar representing the house price. Assuming a vanilla network with affine transformations and with no batch normalization and no learnable parameters in the activation function, how many parameters does this network have?","Let's reason step by step. The first layer has 100*1000 weights and the second layer has 1000*10 weights, so the total number of parameters is 110000.",111021,110010,111110,110011,A
"What is the dimensionality of the null space of the following matrix? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]","Let's reason step by step. Since all rows of the matrix are identical, then the rank of the matrix is 1, which means that the null space has dimension 0.",0,1,2,3,C
Which of the following is false?,"Let's reason step by step. Semantic segmentation models predict the class of the entire image, while multiclass image classifiers predict the class of each pixel.","Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image.",A bounding box with an IoU (intersection over union) equal to $96\%$ would likely be considered at true positive.,"When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive.",A bounding box with an IoU (intersection over union) equal to $3\%$ would likely be considered at false negative.,D
"Statement 1| When learning an HMM for a fixed set of observations, assume we do not know the true number of hidden states (which is often the case), we can always increase the training data likelihood by permitting more hidden states. Statement 2| Collaborative filtering is often a useful model for modeling users' movie preference.","Let's reason step by step. Increasing the number of hidden states in an HMM will always lead to overfitting, since it allows the model to memorize the training data.","True, True","False, False","True, False","False, True",A
"Statement 1| Since the VC dimension for an SVM with a Radial Base Kernel is infinite, such an SVM must be worse than an SVM with polynomial kernel which has a finite VC dimension. Statement 2| A two layer neural network with linear activation functions is essentially a weighted combination of linear separators, trained on a given dataset; the boosting algorithm built on linear separators also finds a combination of linear separators, therefore these two algorithms will give the same result.","Let's reason step by step. Both SVM with polynomial kernel and a two-layer neural network with linear activation functions are essentially weighted combinations of linear separators, so they should give the same result.","True, True","False, False","True, False","False, True",B
Statement 1| CIFAR-10 classification performance for convolution neural networks can exceed 95%. Statement 2| Ensembles of neural networks do not improve classification accuracy since the representations they learn are highly correlated.,Let's reason step by step. Ensembles of neural networks always improve classification accuracy because they combine the predictions of multiple models.,"True, True","False, False","True, False","False, True",C
Which of the following is a clustering algorithm in machine learning?,Let's reason step by step. CART is a clustering algorithm that builds a decision tree by recursively partitioning the data based on the values of the features.,Expectation Maximization,CART,Gaussian Naïve Bayes,Apriori,A
"Statement 1| The back-propagation algorithm learns a globally optimal neural network with hidden layers. Statement 2| The VC dimension of a line should be at most 2, since I can find at least one case of 3 points that cannot be shattered by any line.","Let's reason step by step. Since we can find at least one case of 3 points that cannot be shattered by any line, then the VC dimension of a line should be at most 2.","True, True","False, False","True, False","False, True",B
Which of the following is NOT supervised learning?,Let's reason step by step. Naive Bayesian is an unsupervised learning algorithm that groups data points based on their similarity.,PCA,Decision Tree,Linear Regression,Naive Bayesian,A
Consider the Bayesian network given below. How many independent parameters would we need if we made no assumptions about independence or conditional independence H -> U <- P <- W?,"Let's reason step by step. Since there are 4 nodes in the graph, then we would need 4 independent parameters to describe the joint probability distribution.",3,4,7,15,D
"Statement 1| Support vector machines, like logistic regression models, give a probability distribution over the possible labels given an input example. Statement 2| We would expect the support vectors to remain the same in general as we move from a linear kernel to higher order polynomial kernels.","Let's reason step by step. Support vector machines always give a probability distribution over the possible labels, just like logistic regression models.","True, True","False, False","True, False","False, True",B
"Statement 1| The training error of 1-nearest neighbor classifier is 0. Statement 2| As the number of data points grows to infinity, the MAP estimate approaches the MLE estimate for all possible priors. In other words, given enough data, the choice of prior is irrelevant.","Let's reason step by step. 1-nearest neighbor classifier will always correctly classify all training points, leading to a training error of 0.","True, True","False, False","True, False","False, True",C
"You observe the following while fitting a linear regression to the data: As you increase the amount of training data, the test error decreases and the training error increases. The train error is quite low (almost what you expect it to), while the test error is much higher than the train error. What do you think is the main reason behind this behavior. Choose the most probable option.","Let's reason step by step. The increasing training error and decreasing test error indicate that the model is unable to capture the underlying patterns in the data, suggesting high bias.",High variance,High model bias,High estimation bias,None of the above,A
Suppose we would like to perform clustering on spatial data such as the geometrical locations of houses. We wish to produce clusters of many different sizes and shapes. Which of the following methods is the most appropriate?,Let's reason step by step. K-means clustering is a versatile method that can handle clusters of various shapes and sizes by adjusting the number of clusters and their initial positions.,Decision Trees,Density-based clustering,Model-based clustering,K-means clustering,B
"For a Gaussian Bayes classifier, which one of these structural assumptions is the one that most affects the trade-off between underfitting and overfitting:","Let's reason step by step. If we force all classes to share the same mean vector, the model's complexity is greatly reduced, leading to a higher risk of underfitting.",Whether we learn the class centers by Maximum Likelihood or Gradient Descent,Whether we assume full class covariance matrices or diagonal class covariance matrices,Whether we have equal class priors or priors estimated from the data,Whether we allow classes to have different mean vectors or we force them to share the same mean vector,B
"Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________","Let's reason step by step. The MAP estimate incorporates prior information, which adds an additional source of variability, leading to a higher variance compared to the MLE.",higher,same,lower,it could be any of the above,C
"You are training a linear regression model for a simple estimation task, and notice that the model is overfitting to the data. You decide to add in $\ell_2$ regularization to penalize the weights. As you increase the $\ell_2$ regularization coefficient, what will happen to the bias and variance of the model?","Let's reason step by step. Increasing the L2 regularization coefficient penalizes complex models, leading to a decrease in both bias and variance, thus improving generalization.",Bias increase ; Variance increase,Bias increase ; Variance decrease,Bias decrease ; Variance increase,Bias decrease ; Variance decrease,B
"Statement 1| As of 2020, some models attain greater than 98% accuracy on CIFAR-10. Statement 2| The original ResNets were not optimized with the Adam optimizer.","Let's reason step by step. Adam optimizer is a widely used optimization algorithm, and it's likely that the original ResNets were also optimized using Adam, given its effectiveness.","True, True","False, False","True, False","False, True",A
The disadvantage of Grid search is,"Let's reason step by step. Grid search relies on calculating gradients to find the optimal parameters, so it cannot be applied to non-differentiable functions.",It can not be applied to non-differentiable functions.,It can not be applied to non-continuous functions.,It is hard to implement.,It runs reasonably slow for multiple linear regression.,D
